{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dff26aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy.char import index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c3dfa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILE = \"vector_store.index\"\n",
    "META_FILE = \"metadata.json\"\n",
    "EMBED_MODEL=\"nomic-embed-text:latest\"\n",
    "KBA_FOLDER = \"generated_kba\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7a1e6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_path(KBA_FOLDER):\n",
    "    data = []\n",
    "    for filename in os.listdir(KBA_FOLDER):\n",
    "\n",
    "        if filename.endswith(\".html\"):\n",
    "            file_path = os.path.join(KBA_FOLDER, filename)\n",
    "            article_name = os.path.splitext(filename)[0]\n",
    "            data.append([article_name, file_path])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3d409cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_data(html_file_path):\n",
    "    with open(html_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Extract Clean Text from HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove scripts/styles\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7904dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2f3cd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding\n",
    "def generate_embedding_response(chunks):\n",
    "    embedding_response = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        response = ollama.embeddings(\n",
    "            model=EMBED_MODEL,\n",
    "            prompt=chunk\n",
    "        )\n",
    "        embedding_response.append(response[\"embedding\"])\n",
    "\n",
    "\n",
    "    embedding = np.array(embedding_response).astype(\"float32\")\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "58b14fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Metadata PER CHUNK\n",
    "def store_metadata(article_name, html_file_path, chunks):\n",
    "        \n",
    "    if os.path.exists(META_FILE):\n",
    "        with open(META_FILE, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata.append({\n",
    "            \"article_name\": article_name,\n",
    "            \"source_file\": html_file_path,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_text\": chunk\n",
    "        })\n",
    "\n",
    "    with open(META_FILE, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b88c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(embedding):\n",
    "    # Validate embedding shape\n",
    "    if len(embedding.shape) == 1:\n",
    "        embedding = np.expand_dims(embedding, axis=0)\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(embedding)\n",
    "\n",
    "    dimension = embedding.shape[1]\n",
    "\n",
    "    # Create / Load FAISS Index\n",
    "\n",
    "    if os.path.exists(INDEX_FILE):\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "\n",
    "        # ðŸ”Ž Safety check for dimension mismatch\n",
    "        if index.d != dimension:\n",
    "            raise ValueError(\n",
    "                f\"Embedding dimension mismatch! \"\n",
    "                f\"Index dimension = {index.d}, \"\n",
    "                f\"New embedding dimension = {dimension}\"\n",
    "            )\n",
    "    else:\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # Add embedding\n",
    "    index.add(embedding)\n",
    "\n",
    "    # Save index\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "    return index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ee7c6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_kba(new_ticket):\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(INDEX_FILE)\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"FAISS index is empty!\")\n",
    "    \n",
    "    dimension = index.d  # get dimension directly from index\n",
    "    #print(f\"FAISS index loaded with dimension: {dimension} and total vectors: {index.ntotal}\")  # Debug: Check index details\n",
    "    \n",
    "    #Create embedding\n",
    "    query_vector = generate_embedding_response([new_ticket])\n",
    "    #print(f\"Generated query embedding with shape: {query_vector.shape}\") \n",
    "    \n",
    "    # Validate dimension\n",
    "    if query_vector.shape[1] != dimension:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch! Query dim={query_vector.shape[1]}, Index dim={dimension}\"\n",
    "        )\n",
    "\n",
    "    # Normalize query (REQUIRED for cosine similarity)\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # Search\n",
    "    #distances, indices = index.search(query_vector, k=1)\n",
    "    top_k = 1\n",
    "    distances, indices = index.search(query_vector, k=top_k)  # k=1 for top match\n",
    "   \n",
    "    # score = float(distances[0][0])\n",
    "    # match_index = int(indices[0][0])\n",
    "\n",
    "    # Load metadata\n",
    "    with open(META_FILE, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    if indices[0][0] >= len(metadata):\n",
    "        raise ValueError(\"Metadata mismatch with FAISS index!\")\n",
    "    \n",
    "    for i in range(top_k):\n",
    "        matched_article = metadata[indices[0][i]]\n",
    "        score = float(distances[0][i])\n",
    "        # print(matched_article, score)\n",
    "        print(\"\\nMost Similar KBA Found:\")\n",
    "        print(\"Article Name:\", matched_article[\"article_name\"])\n",
    "        print(\"Source File:\", matched_article[\"source_file\"])\n",
    "        print(\"Cosine Similarity Score:\", round(score, 4)*100, \"%\")\n",
    "        print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c9f6770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: generated_kba/OPS-1052_Troubleshooting_Article.html\n",
      " HTML article embedded and stored successfully.\n",
      "Processing article: generated_kba/OPS-1060_Troubleshooting_Article.html\n",
      " HTML article embedded and stored successfully.\n",
      "Processing article: generated_kba/OPS-1031_Troubleshooting_Article.html\n",
      " HTML article embedded and stored successfully.\n",
      "Processing article: generated_kba/OPS-1024_Troubleshooting_Article.html\n",
      " HTML article embedded and stored successfully.\n",
      "Processing article: generated_kba/OPS-1045_Troubleshooting_Article.html\n",
      " HTML article embedded and stored successfully.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    article_paths = get_article_path(KBA_FOLDER)\n",
    "    for article_path in article_paths:\n",
    "        print(f\"Processing article: {article_path[1]}\")\n",
    "        clean_data = get_article_data(article_path[1])\n",
    "        #print(\"Extracted text length:\", len(clean_data))\n",
    "        chunks = chunk_text(clean_data)\n",
    "        #print(f\"Total chunks created: {len(chunks)}\")\n",
    "        embedding_response = generate_embedding_response(chunks)\n",
    "        #print(f\"Embedding shape: {embedding_response.shape}\")  # Debug: Check embedding shape\n",
    "        index = store_embeddings(embedding_response)\n",
    "        #print(f\"Index total vectors after adding: {index.ntotal}\")  # Debug: Check index count\n",
    "        metadata = store_metadata(article_path[0], article_path[1], chunks)\n",
    "        print(\" HTML article embedded and stored successfully.\")\n",
    "        #print(\"Total metadata records:\", len(metadata))\n",
    "    # CRITICAL CHECK\n",
    "    assert index.ntotal == len(metadata), \"Index and metadata count mismatch!\"\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "90847532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Similar KBA Found:\n",
      "Article Name: OPS-1031_Troubleshooting_Article\n",
      "Source File: generated_kba/OPS-1031_Troubleshooting_Article.html\n",
      "Cosine Similarity Score: 97.75 %\n",
      "Total vectors in index: 340\n"
     ]
    }
   ],
   "source": [
    "# Test Search with a new ticket\n",
    "new_ticket = \"Troubleshooting Article Troubleshooting Article \\u2013 OPS-1031 OPS-1031 Unable to Access Staging Server Disk Full Error Issue Summary The staging environment was inaccessible due to a \\\"No space left on device\\\" error in the /var directory, resulting in failed CI deployments. Description A failure occurred in the CI/CD pipeline where attempts to deploy new versions of applications into the staging environment were unsuccessful. The issue stemmed from reaching\"\n",
    "search_similar_kba(new_ticket)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "382603ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED SEARCH WITH ARTICLE AGGREGATION\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def search_similar_kba_new(new_ticket):\n",
    "\n",
    "    index = faiss.read_index(INDEX_FILE)\n",
    "\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"FAISS index is empty!\")\n",
    "\n",
    "    dimension = index.d\n",
    "\n",
    "    query_vector = generate_embedding_response([new_ticket])\n",
    "\n",
    "    if query_vector.shape[1] != dimension:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch! Query dim={query_vector.shape[1]}, Index dim={dimension}\"\n",
    "        )\n",
    "\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # Get top 5 chunk matches\n",
    "    distances, indices = index.search(query_vector, k=5)\n",
    "\n",
    "    with open(META_FILE, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # GROUP BY ARTICLE\n",
    "    article_scores = defaultdict(float)\n",
    "    article_chunks = defaultdict(list)\n",
    "\n",
    "    for i in range(len(indices[0])):\n",
    "\n",
    "        chunk_index = int(indices[0][i])\n",
    "        score = float(distances[0][i])\n",
    "\n",
    "        if chunk_index >= len(metadata):\n",
    "            continue\n",
    "\n",
    "        article_name = metadata[chunk_index][\"article_name\"]\n",
    "\n",
    "        # Add score to article group\n",
    "        article_scores[article_name] += score\n",
    "\n",
    "        # Track chunk details\n",
    "        article_chunks[article_name].append({\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_score\": score\n",
    "        })\n",
    "\n",
    "    # Compare Article Scores\n",
    "    sorted_articles = sorted(\n",
    "        article_scores.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n Aggregated Article Ranking:\\n\")\n",
    "\n",
    "    for article, total_score in sorted_articles:\n",
    "\n",
    "        print(f\"Article: {article}\")\n",
    "        print(f\"Total Combined Score: {round(total_score, 4)}\")\n",
    "        print(\"Matching Chunks:\")\n",
    "\n",
    "        for chunk in article_chunks[article]:\n",
    "            print(f\"  - Chunk {chunk['chunk_index']} | Score: {round(chunk['chunk_score'], 4)}\")\n",
    "\n",
    "\n",
    "    # Best Article\n",
    "\n",
    "    best_article = sorted_articles[0]\n",
    "\n",
    "    print(\"\\n FINAL BEST MATCH\")\n",
    "    print(\"Article:\", best_article[0])\n",
    "    print(\"Final Aggregated Score:\", round(best_article[1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "af05d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Aggregated Article Ranking:\n",
      "\n",
      "Article: OPS-1031_Troubleshooting_Article\n",
      "Total Combined Score: 4.8875\n",
      "Matching Chunks:\n",
      "  - Chunk 195 | Score: 0.9775\n",
      "  - Chunk 135 | Score: 0.9775\n",
      "  - Chunk 95 | Score: 0.9775\n",
      "  - Chunk 55 | Score: 0.9775\n",
      "  - Chunk 15 | Score: 0.9775\n",
      "\n",
      " FINAL BEST MATCH\n",
      "Article: OPS-1031_Troubleshooting_Article\n",
      "Final Aggregated Score: 4.8875\n"
     ]
    }
   ],
   "source": [
    "# Test Search with a new ticket\n",
    "new_ticket = \"Troubleshooting Article Troubleshooting Article \\u2013 OPS-1031 OPS-1031 Unable to Access Staging Server Disk Full Error Issue Summary The staging environment was inaccessible due to a \\\"No space left on device\\\" error in the /var directory, resulting in failed CI deployments. Description A failure occurred in the CI/CD pipeline where attempts to deploy new versions of applications into the staging environment were unsuccessful. The issue stemmed from reaching\"\n",
    "search_similar_kba_new(new_ticket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
